%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
%%
\documentclass[sigconf]{acmart}

\usepackage{multirow}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Retrieve–Revise–Refine with Clause-level Retrieval for Legal Statute Entailment}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Bhoomi Goyal}
\email{bhoomig@iitbhilai.ac.in}
\affiliation{%
  \institution{Indian Institute of Technology}
  \city{Bhilai}
  \state{Chhattisgarh}
  \country{India}
}

\author{Soni Kumari}
\email{sonik@iitbhilai.ac.in}
\affiliation{%
  \institution{Indian Institute of Technology}
  \city{Bhilai}
  \state{Chhattisgarh}
  \country{India}
}

\author{Shivangi Gaur}
\email{shivangig@iitbhilai.ac.in}
\affiliation{%
  \institution{Indian Institute of Technology}
  \city{Bhilai}
  \state{Chhattisgarh}
  \country{India}
\email{larst@affiliation.org}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Computational law Central to retrieving statutory entitlement Statutory retrieval This is a central task in computational law, as demonstrated in events like the COLIEE competition, where systems are required to retrieve not only statutes which are relevant but also a succinct combination of statutes that entails or refutes a legal hypothesis. Even those systems that are currently in place such as earlier deep-learning retrieval pipelines tend to scale on an article level and this results in semantic dilution since legal articles usually have many clauses in them, all of which are not logically salient. In order to handle this, we add to the Retrieve-Revise-Refine (RRR)\cite{NGUYEN2025103949} model to create a clause-level retrieval and clausewise aggregation. Retrieve stage is reformulated to divide each statute into clauses and rank them with the help of a hybrid sparse-dense retrieval architecture. The Revise stage incorporates prompting of large language models informed by clauses to achieve lower error rates on over-selection. Lastly, the Refine phase has the inclusion of clause-evidence boosting to enhance ranking and conciseness. We test on COLIEE datasets and see that our system improves macro-F2 and interpretability. The design on the clause level provides finer-grained justification, enhanced retention of significant statutory fragments and lower final retrieved sets of articles.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%


\ccsdesc[500]{Information systems~Information retrieval; Retrieval models and ranking;}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Legal Information Retrieval; Large Language Models; COLIEE}

\maketitle

\section{Introduction}
Artificial Intelligence (AI) continues to redefine the boundaries of legal technology, offering promise in automating advanced tasks such as legal question answering and consultation.1 Within the domain of statute law, a principal challenge is the task of retrieving the concise set of entailing legal articles for a given query. This task, formally referred to as entailing legal article set retrieval, is essential for building reliable, high-level legal reasoning applications.1The objective of this task is to read a legal statement (the query, $Q$) and identify a concise—that is, precise and compact—set of legal articles $\{A_1, A_2,..., A_n\}$ from a corpus that holds a joint entailment relationship with $Q$ or its negation, $\neg Q$.1

This is a major departure of the traditional ad-hoc information retrieval (IR). Conventional IR models are tuned to rank the individual documents by the relevance of topicality of the documents, returning a long list of candidates. Comparatively, such a set-based retrieval problem is the retrieval of a minimum set of articles, such that the failure of any one article to appear in the retrieval result can result in a wrong logical conclusion.1The set retrieval problems described are exemplified in Figure 1 of the work by Nguyen et al. (2025) where a query in the area of extinctive prescription to tort claims is not answered by a single article, but is instead negated by the specific interaction between a general rule (Article 724) and its specific modifier (Article 724-2).

Multi-stage architecture Due to the peculiarities of the COLIEE task, the state-of-the-art has been a multi-stage architecture called Retrieve-Revise-Refine (RRR) framework. The framework attained absolute increases on the macro F2 score of 3.17\% on the COLIEE 2022 dataset and 4.24\% on the COLIEE 2023 datasets, compared to the past SOTA methods, indicating its usefulness.

\subsection{State-of-the-Art: The Retrieve-Revise-Refine (RRR) Framework}

RRR \cite{NGUYEN2025103949} is a complex, three-step pipeline using the synergies between the limited strengths of small and specialized language models (LMs) and large-scale and general-purpose language models (LLMs) to generating strategies:

Retrieve: This is accomplished by a collection of several fine-tuned small LMs to maximize the coverage and retrieve a rich set of candidate articles. This level is very effective with a recall rate of more than 0.90 in the top-5 results only.
\begin{itemize}
    \item \textbf{Revise:} Large LMs are subsequently used as not re-rankers, but as validators. They evaluate the query in terms of validity relative to different combinations of the articles retrieved to make a smaller and more narrow subset.
    \item \textbf{Refine:} Lastly, the knowledge gained with the small LMs is applied to cross-check the predictions of the large LMs, and narrow down the output again to the most succinct and precise set.
    \item Refine: Lastly, the knowledge gained with the small LMs is applied to cross-check the predictions of the large LMs, and narrow down the output again to the most succinct and precise set.
\end{itemize}

The hybrid architecture is an important step forward and it offers a powerful approach to the balancing of the high-recall needs of the initial search and the high-precision needs of the final set.

\subsection{The "Dilution of Meaning" Gap: Limitations of Article-Level Processing}
Although proven to be successful in practice, there is underlying architecture that limits the performance of the RRR framework: its retrieval stage, similar to most previous activity in the COLIEE competition, is article-level. This large-grained retrieval unit generates a limit to performance introducing two serious issues, whose combination is known as the gap of the so-called dilution of meaning:
\begin{enumerate}
    \item \textbf{Dilution of Meaning:} Legal documents tend to be lengthy and heterogeneous, with a number of clauses or provisions. In the case of a single clause that is very relevant to a query, the signal of the clause is strongly averaged with the noise of a large number of irrelevant clauses in the same article. This dilution has the potential to reduce the overall relevance score of the article which harms retrieval precision and retrieval.
    \item \textbf{Loss of Recall:} In the inverse case, an important entailing clause can be concealed within a long article which dominates on other matters. The overall semantic representation of the article might not be similar to the query, so it would be ranked too low to be included in the initial candidate set, and the appropriate article would be lost permanently.
\end{enumerate}

The multi-phase Revise and Refine aspects of the RRR framework, though strong, are in fact computationally expensive devices which are aimed at counterbalancing the inaccuracy which is inherent in this first step, at the article level. The pipeline must re-read a loud, over- covering set, and consequently spend a lot of resources on damage control and signal finding in the downstream phases.


\subsection{Our Contributions: A Fine-Grained, Clause-Aware RRR Framework}

It is suggested in this paper that the better and more efficient solution is not to put the right solution over and over, but to repair the base. We propose a new framework that improves all three steps of the RRR pipeline and makes them clause-aware, which is a direct answer to the gap of the dilution of meaning. With a smaller granularity, our framework has a far purer, more accurate signal to the downstream reasoning and validation phases.

The main works of this work are:
\begin{enumerate}
    \item \textbf{A Fine-Grained Retrieve Stage}: Article-level retrieval is substituted by a new pipeline that initially carries out Clause Segmentation of the legal corpus. Then it uses Hybrid Retrieval at the clause-level, which consists of both sparse (e.g., BM25\cite{chen2023bm25queryaugmentationlearned}, SPLADE \cite{formal2021spladesparselexicalexpansion}) and dense (e.g., LegalBERT) vectors to represent both lexical and semantic relevance. Lastly, Clause-aware Aggregation strategy is a strategy that integrates these signal-based fine-grained signals to generate a higher article-level ranking
    \item \textbf{An Informed Revise Stage:} The stage of validation with the LLM is greatly improved. In the Clause-informed Prompting, the LLM is not presented with the entire text of candidate articles (the haystack), but only with the high scoring clauses that were specifically high-ranking (the needles). This puts the rationale of the LLM on direct evidence. This is augmented by a Light Entailment Verifier a smaller, dedicated model that cross verifies the decisions of the LLM to a logical consistency.

    \item \textbf{A Clause-Based Refine Phase}: This refinement step is substituted by a more specific process. Clause-evidence Boosting This is a new heuristic, which in the final re-ranking assigns higher scores to articles supported by many (distinct) entailing clauses. This heuristic directly punishes articles which offer a more complete body of evidence, which formally matches the set-based nature of the task to which it is applied.
\end{enumerate}

\section{Related Works}
In order to frame the contributions of this project, the following section takes an exhaustive literature review, addressing the history of the retrieval techniques of the COLIEE task, the main technologies on which our novel and fine-grained architecture is based.

\subsection{Legal Article Retrieval in COLIEE Evolution}
The strategies of the COLIEE statute law retrieval task have been developed in three successive stages which clearly have a clear and steady trend of increasingly complicated multi-stage pipelines to deal with the difficulty of the task.

\textbf{Phase 1 Classical IR and Early Ensembles}. The initial successful COLIEE systems were based on non-semantic IR models. The techniques of HUKB and UA used sets of the keyword-based models, like TF-IDF and BM25, frequently together with hand-crafted feature selection based on domain knowledge. Although they work well with queries that have a high level of lexical overlap, such techniques were by their nature unable to capture the semantic subtleties of legal language, where two or more phrases can describe a single legal phenomenon.

\textbf{Phase 2: Deep Learning and BERT-based Models}. The second step involved the implementation of deep learning models that are fine-tuned. Specialized deep learning models that were specific to various types of questions were created by the JNLP team, e.g. CAPTAIN, a former state of the art holder used ensembles of the BERT-based models and the monoT5 sequence to sequence model to improve the capture of semantic relationships. These methods greatly enhanced semantic matching, however, they were still done on a per article level and therefore could be subjected to the issue of dilution of meaning.

\textbf{Phase 3: Multi-Stage Pipelines and LLMs.} The third stage is the present SOTA, which comprises the RRR baseline. These systems acknowledge the drawbacks of the single-stage retrieval model and adopt complicated pipelines. This typically consists of retrieval (sparse or dense) and then a strong, and computationally costly, re-ranking or validation step using an LLM. These systems rely on LLMs to verify relevance, confirm combinations and filter the first stage that is noisy (article-level). \cite{nguyen2024enhancinglegaldocumentretrieval}\cite{nguyen2024exploitingllmsreasoningcapability}.

This historical development, as it is summarized in Table 1, shows a case of critical trend. The field has so far been stagnant at the article level, unable to add more complicated and expensive layers to the pipeline to compensate the inaccuracy of the underlying retrieval step. In this project, the next logical step, which is not to add another layer, but to correct the basis, is, however, the first to propose it.

\begin{table*}
   
  \caption{Development of the State-of-the-Art Methodologies in COLIEE Statute Law Retrieval}
  \label{tab:retrieval_methods}
  \begin{tabular}{1111}
    \toprule
    Method / Team & Core Methodology & Retrieval Unit & Key Limitation \\
    \midrule
    HUKB (2022)\cite{NGUYEN2025103949} & BM25 / Classical IR Ensemble & Article & No semantic understanding; Article-level. \\
    JNLP \cite{nguyen2021jnlpteamdeeplearning}(2023) & Specialized DL Models & Article & Article-level dilution of meaning. \\
    CAPTAIN \cite{nguyen2024captaincoliee2023efficient} (2023) & monoT5 / BERT Ensemble & Article & Article-level dilution of meaning. \\
    RRR (2025) \cite{NGUYEN2025103949}& Small LM + LLM Pipeline & Article & Corrective pipeline for article-level noise. \\
    Our Work & Fine-Grained Hybrid RRR & Clause & Addresses dilution at the source. \\
    \bottomrule
  \end{tabular}
\end{table*}


\subsection{Fine-Grained Retrieval in Law and Complicated Domains}

The idea to employ Clause Segmentation is based on a solid theoretical foundation of research in general IR and is especially applicable to the legal field. This is a basic issue of Long Document Retrieval (LDR). The IR community has long known that in the case of LDR tasks, such aggregation of smaller text units (passages or sentences) is a very effective way to rank full documents.

This classical concept has undergone a resurgence in the contemporary IR, especially the Retrieval-Augmented Generation (RAG). such systems as DSLR ( Document Refinement with Sentence-Level Re-ranking and Reconstruction) show that decomposing documents into sentences to filter and re-rank can dramatically improve the performance of RAG pipelines by giving better cleaner and relevant context to the LLM.

In the Legal AI community, this granular method has already been deemed necessary, though has been extended to NLP tasks more than IR. An example is a study on automated compliance, which has concentrated on determining the existence of unfair clauses at the sentence level in Terms of Service. Equally, contract analysis task aims at slaughtering and monitoring specified provisions in non-disclosure agreements (NDAs). Other literature has shown the relevance of a fine-grained analysis in classifying intents of legal case document.

There is a remarkable disagreement: an NLP aspect of law (compliance, analysis) has adopted clause-level processing as an important one, but the legal IR aspect (COLIEE retrieval) has remained mostly at the level of an article. This paper fills that gap, by syntactically combining the finedetail methods of NLP of law in a state-of-the-art legal retrieval pipeline (RRR) to accomplish a task traditionally regarded as document-level.

The third type of search is the hybrid search involving the combination of sparse and dense representations.
In order to carry out retrieval on the level of clause, this project suggests Hybrid Retrieval. This is done because it is through sparse and dense vectors that are complementary and therefore very essential in the law world.
\subsection{Hybrid Search: Integrating Sparse and Dense Representations}

The third type of search is the hybrid search involving the combination of sparse and dense representations.
In order to carry out retrieval on the level of clause, this project suggests Hybrid Retrieval. This is done because it is through sparse and dense vectors that are complementary and therefore very essential in the law world.

\subsubsection{Sparse Retrieval (BM25 \& SPLADE)}
The lexical matching based sparse retrieval models are nonetheless necessary in the legal texts where the accuracy of a particular word (e.g., "Article 724," "tortious act") is beyond negotiation. BM25, an efficient and powerful baseline is used to match with this very word.

Nevertheless, recent developments have yielded learned sparse models at even better representations. The variants of SPLADE are the contextualized sparse document encoders which learn to increase representation of a document and add weighted relevant terms. This offers better representation than BM25 and still has the invaluable advantages of perfect matching and interpretability that dense models do not possess. The adoption of the state-of-the-art sparse technology is indicated by the plan to use SPLADE.

\subsubsection{Dense Retrieval (LegalBERT)}

Transformer-based embeddings are used to solve the so-called vocabulary mismatch problem by dense retrieval, which aims at capturing semantic meaning. A query of compensation for harm must include a document of damages for loss though they may not be related by the keywords. This plays a crucial role in the area of law. The suggested methodology also rightly cites the necessity of an application-specific model such as LegalBERT that is already pre-trained on a large legal dataset and therefore has a more subtle knowledge of legal terminology that a generic BERT implementation does not.

\subsubsection{The Case for Hybrid Models}

The legal field needs the sparse model precision aspect of the keyword and the dense model meaning. Hence, hybrid system is not only an option, but a necessity. Studies have continuously demonstrated that hybrid systems, comprising of the score of sparse and dense retrievers (typically through a weighted combination or Reciprocal Rank Fusion), are superior to either.

Moreover, the literature demonstrates apparently disputable conclusions on which approach proves to be the best one. According to some studies, dense representations are significantly weaker than sparse representations and dense embeddings alone do not perform well particularly when it comes to long, noisy queries in the legal context. Other work on the contrary has discovered that the precision/recall of BM25 can be dropped by the settings. This is not a contradiction but it is a great evidence that optimal balance is task and dataset-dependent. The only solution to this trade-off and high performance on all types of queries is to have a strong hybrid architecture as suggested in this project.

\subsection{ Legal Re-Ranking and Legal Verification LLM}
The suggested improvements of the Revise phase are based on the recent findings concerning the re-ranking based on LLM and the concept of task disentanglement.

\subsubsection{Clause-Informed Prompting} As cross-encoders or applied in generating re-ranking, LLLMs have been demonstrated to be state-of-the-art in terms of precision. LLM are already used as validators by the RRR baseline. Clause-informed Prompting is the main innovation suggested in this paper.

The initial RRR model uses a needle in a haystack strategy: the query is given to the LLM in combination with one or more entire articles, and the costly model must first locate the evidence that it is interested in inside the text before it is able to do the validation task. This is a poor and imprecise utilization of the context window of the model.

The suggested strategy is an improved version of RAG. The system bases the reasoning of the LLM on the direct evidence by providing the query, the article, and the particular and high scoring clauses found by the fine-grained retriever. This makes the task a closed-ended search-and-check, instead of a far easier and more dependable validate-this-evidence, and this ought to significantly enhance the accuracy and efficiency of the Revise stage.

\subsection{Retrieval to Ranking: Evidence Boosting}

The last element of the proposed framework is the Boosting heuristic of Clause evidences to the Refine phase. This is a new re-ranking heuristic, which can only be enabled by the clause-level retrieval architecture.

Identifying a concise set of articles is the task as stipulated by COLIEE and the RRR paper. The system, having undergone the initial retrieval and revision, contains a collection of candidate articles out of which each article is backed by one or more retrieved clauses. The Clause-evidence Boosting heuristic postulates that an article that is backed by several dissimilar entailing clauses is a more robust and central part of the legal argument compared to an article that is backed by a basic one. This hub article is further likely to be a critical article of the ultimate set of entailing.

In the case of a query, for instance, the query can contain relevant clauses in a general rule (such as Art. 724) and an exception (such as Art. 724-2) to it. The inclusion of several such clauses in an article is an extremely positive indicator. It is this direct optimization of the set and conciseness requirements of the task by increasing such articles in the final re-ranking that makes this heuristic effective at removing the need to solve the problem at an article level. To build a better and more complete final article set, it uses the fine-grained evidence as a meta-feature.

\section{Methodology}
\section{Methodology: The RRR Baseline}

 \begin{figure}
     \centering
     \includegraphics[width=1.15\linewidth]{3.png}
     \caption{Overall stages within our Retrieve–Revise–Refine framework for legal article set retrieval. This diagram outlines the three main stages of our approach:
Retrieval stage employs small LMs, Revision stage utilizes large LMs, and Refinement stage combines insights from both model types.\cite{NGUYEN2025103949}}
 \end{figure}
Our project is a direct extension and enhancement of the state-of-the-art \textbf{Retrieve-Revise-Refine (RRR)} framework proposed by Nguyen et al. The RRR framework was specifically designed to address the unique challenge of the COLIEE task: retrieving a \textit{concise set} of entailing articles, rather than a simple ranked list of relevant documents. It achieves this by strategically employing small language models (sLMs) for high-recall retrieval and large language models (LLMs) for high-precision validation and reasoning.

The baseline RRR methodology, which operates at the \textbf{article level}, is divided into three distinct stages, as illustrated in Figure~2 of the original work.

\subsection{Stage 1: Retrieve (Small LMs as Retrievers)}

The primary goal of the first stage is to maximize recall, ensuring that all potentially entailing articles are captured in the initial candidate set. The baseline framework treats this as a traditional retrieval task, ranking articles by their relevance to the query.

The process is as follows:
\begin{enumerate}
    \item \textbf{Data Construction:} The model is fine-tuned on positive and negative pairs. Positive pairs consist of a query and its known entailing articles. Negative pairs are generated using a TFIDF-based negative sampling strategy, pairing the query with top-$k$ non-entailing articles.
    \item \textbf{Specialized Fine-Tuning:} A BERT-based sLM is fine-tuned using a multi-tiered approach to handle specific data challenges:
    \begin{itemize}
        \item \textbf{Initial Fine-Tuning:} Creates the base model (Algorithm~1).
        \item \textbf{Continual Fine-Tuning:} Addresses ``null-result queries'' where the base model fails to retrieve any articles (Algorithm~2).
        \item \textbf{Bootstrap Fine-Tuning:} Mitigates confusion between articles with highly similar content (Algorithm~3).
    \end{itemize}
    \item \textbf{Ensemble for Coverage:} The final retrieval list is an ensemble of 15 checkpoints (five from each fine-tuning strategy). This ensemble approach is highly effective, achieving a recall rate of over 0.90 within the top-5 distinct results alone.
\end{enumerate}

\subsection{Stage 2: Revise (Large LMs as Revisers)}

The Revise stage uses the reasoning capabilities of LLMs (e.g., Orca-2, Qwen, Mistral 7B) to prune the high-recall set from Stage~1 and identify a more concise subset.

This stage simplifies the task for the LLM by shifting from \textit{selection} to \textit{validation}:
\begin{enumerate}
    \item \textbf{Candidate Generation:} The system selects the top-$k$ distinct articles from the retrieval list.
    \item \textbf{Combination Validation:} It generates all possible combinations of these top-$k$ articles (i.e., $2^k - 1$ combinations).
    \item \textbf{LLM Prompting:} Each combination is fed to an LLM using few-shot Chain-of-Thought (CoT) prompting. The LLM is asked to validate if the \textit{given set} of articles is sufficient to verify the legal statement as ``true,'' ``false,'' or ``not enough information.''
    \item \textbf{Answer Unification:} The system collects all article combinations that the LLM marked as ``true'' or ``false'' (i.e., sufficient). It then identifies the most concise (smallest) set among these valid combinations as the revised output.
\end{enumerate}

\subsection{Stage 3: Refine (Small LMs as Refiners)}

The final Refine stage is designed to cross-validate the LLM's predictions and filter out errors, further enhancing precision. The baseline RRR paper identifies that while individual LLMs perform well, a simple ensemble of their outputs can decrease precision.

The refinement logic is based on \textit{agreement} between the sLMs and LLMs:
\begin{enumerate}
    \item \textbf{LLM Set Ensemble:} The revised sets from all LLMs (e.g., Orca, Qwen, Mistral) are merged to form a consolidated candidate set.
    \item \textbf{sLM Refiner Set:} The system generates a ``refiner set'' by taking the top-$r$ (e.g., $r=6$) predictions from the original 15 sLM checkpoints in Stage~1.
    \item \textbf{Agreement Check:} The final output is the \textit{intersection} of the merged LLM set and the sLM refiner set. An article is only kept if it is deemed relevant by \textit{both} the LLM validation (Stage~2) and the initial sLM retrieval (Stage~1).
\end{enumerate}

\section{Proposed Clause-Aware RRR Framework}

The baseline RRR framework demonstrates state-of-the-art performance, but its methodology is fundamentally constrained by its retrieval unit: the \textbf{entire article}. As identified in our problem statement, this coarse-grained approach leads to critical issues of ``dilution of meaning'' and ``loss of recall,'' where the relevance signal of a single key clause is obscured by the noise of a long, multi-topic article.

Our project addresses this foundational gap. We propose a novel, fine-grained framework that enhances all three stages of the RRR pipeline by making them \textbf{clause-aware}. This approach modifies the core logic of each stage to operate at a more precise, evidence-based level.

\subsection{Stage 1 (Retrieve): Fine-Grained Hybrid Retrieval}

The core of our novelty lies in replacing the article-level retrieval of the baseline with a sophisticated, clause-level hybrid system. This directly attacks the ``dilution of meaning'' problem by isolating relevant signals.

\begin{enumerate}
    \item \textbf{Clause Segmentation:} As a foundational preprocessing step, the entire legal corpus (768 articles) is segmented into its constituent fine-grained units, such as clauses or sentences. This decomposition is a well-established technique in Long Document Retrieval (LDR) and complex Retrieval-Augmented Generation (RAG) pipelines, allowing evidence to be aggregated from smaller, more topically-focused text units.
    \item \textbf{Hybrid Retrieval:} We then perform retrieval at the \textit{clause level} using a hybrid approach that combines the complementary strengths of sparse and dense retrievers. This is critical in the legal domain, which demands both the semantic understanding of dense models and the lexical precision of sparse models.
    \begin{itemize}
        \item \textbf{Sparse Retrieval (BM25/SPLADE):} We use a sparse retriever, such as BM25 or the more advanced SPLADE model. These models excel at exact keyword matching, which is non-negotiable for specific legal terms, article numbers, or named entities. SPLADE, as a learned sparse model, offers a richer, contextualized term representation than BM25, further improving keyword-based matching.
        \item \textbf{Dense Retrieval (LegalBERT):} We simultaneously use a domain-adapted dense retriever like LegalBERT (or similar BERT-based encoders). This is essential for capturing semantic similarity and overcoming the ``vocabulary mismatch problem,'' where a query and a relevant clause use different terminology to express the same legal concept.
    \end{itemize}
    \item \textbf{Clause-aware Aggregation:} After retrieving and scoring individual clauses, the relevance scores are aggregated to produce a final \textit{article-level} rank. Our implementation employs a specific aggregation logic: \textbf{Boosting for articles with multiple clauses.} This function ensures that articles supported by multiple, distinct, high-scoring clauses receive a higher overall score, directly countering the ``dilution of meaning'' problem and surfacing the most evidentially-strong articles.
\end{enumerate}

\subsection{Stage 2 (Revise): Clause-Informed Validation}

We enhance the baseline's Revise stage by providing the LLM with the fine-grained evidence discovered during retrieval. This modification directly addresses the ``needle in a haystack'' problem of the original implementation, where the LLM had to find the relevant evidence within the full article text before performing validation.

Our enhanced stage introduces two key novelties:
\begin{enumerate}
    \item \textbf{Clause-informed Prompting:} Instead of only providing the full article text, our prompt is augmented to include the specific, high-scoring clause(s) that triggered the retrieval. This grounds the LLM's reasoning in direct, relevant evidence, transforming the task from an open-ended ``search-and-validate'' to a much simpler and more reliable ``validate-this-evidence'' task. This technique is technically implemented in our pipeline via the \texttt{revise\_fewshot\_qwen.py} script and is activated by the \texttt{--use\_clauses} parameter.
    \item \textbf{Light Entailment Verifier:} To improve robustness and mitigate LLM errors, we introduce a secondary check. After the generative LLM (like Qwen) validates a set, a small, specialized entailment model (e.g., a fine-tuned DeBERTa trained on entailment tasks) is used to perform a strict textual entailment check. This ``separation of concerns'' uses an expert model to filter out plausible-sounding but logically non-entailing articles, addressing the known difficulties of LLMs with strict legal inference. This feature is implemented in our \texttt{revise\_enhanced.py} script and enabled by the \texttt{--use\_verifier} parameter.
\end{enumerate}

\subsection{Stage 3 (Refine): Clause-Evidence Boosting}

In accordance with our project's focus, we replace the baseline's sLM-agreement refinement logic with a novel heuristic that directly leverages our fine-grained retrieval signals.

The baseline's Refine stage uses agreement with the sLM ensemble to improve precision. Our approach replaces this with \textbf{Clause-evidence Boosting}, a heuristic only possible due to our clause-level architecture.

After the Revise stage, we have a set of candidate articles. In the final re-ranking, our model applies a boosting factor: articles that were supported by \textit{multiple} distinct entailing clauses during the Stage~1 retrieval receive a higher final score. This heuristic is based on the legal reasoning principle that an article providing a more complete body of evidence (i.e., multiple relevant provisions) is more likely to be a critical component of the final, concise \textit{set}. This is technically implemented in our \texttt{refine.py} script and enabled with the \texttt{--use\_boosting} parameter.



\section{Results}
\label{sec:results}

\subsection{Overview and evaluation setting}
We evaluate the proposed Clause-Aware Retrieve–Revise–Refine (RRR) framework on the COLIEE 2025 dataset that was accessible to our team. While previous work by Nguyen et~al.\ (2025) reported detailed performance for COLIEE 2022 and 2023, our version of the dataset contains a different distribution of questions, including a larger number of long-form statute queries and more complex paraphrased entailments. These factors make the dataset more challenging and representative of contemporary retrieval-augmented reasoning tasks.

The Clause-Aware RRR pipeline follows the three-stage architecture of the original RRR framework but modifies each component to operate at the \emph{clause} level rather than at the \emph{article} level. All experiments were conducted using our re-implementation of the RRR codebase (as provided in the public repository) with our additions for clause segmentation, hybrid retrieval, and clause-evidence refinement. Complete experimental commands, checkpoints, and evaluation logs are archived in our project documentation. :contentReference[oaicite:0]{index=0}

We report standard information retrieval metrics: macro-averaged Precision ($P$), Recall ($R$), and the $F_2$ score, where recall is weighted twice as heavily as precision. This weighting aligns with the COLIEE task goal of maximizing coverage of entailing statutes while maintaining reasonable precision. All numbers are averaged over the validation queries available in the COLIEE 2025 release.

\subsection{Quantitative results}
Table~\ref{tab:rrr_results} shows the results after each stage of our pipeline.

\begin{table*}[htbp]
\centering
\caption{Comparative performance between the Clause-Aware RRR framework (evaluated on COLIEE 2025) and the original RRR baseline (COLIEE 2023) reported by Nguyen et al. All values are macro-averaged. (\% change) columns indicate relative improvement or degradation with respect to the baseline. The Clause-Aware model operates at clause granularity in Stage 1 and propagates fine-grained evidence through later stages.}
\label{tab:comparison-rrr}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Stage}} & 
\multicolumn{3}{c}{\textbf{RRR Baseline (COLIEE 2023)}} &
\multicolumn{3}{c}{\textbf{RRR (COLIEE 2025)}} &
\multicolumn{2}{c}{\textbf{Δ (\%) vs Baseline}}\\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-9}
 & Precision & Recall & F$_2$ & Precision & Recall & F$_2$ & Precision Δ & F$_2$ Δ \\
\midrule
Retrieve & 0.2280 & \textbf{0.9250} & 0.5598 & 
\textbf{0.6772} & 0.8011 & \textbf{0.7420} &
+197.0 \% & +32.5 \% \\
\addlinespace
Revise & 0.5733 & 0.8950 & 0.7772 & 
0.6113 & 0.7302 & 0.6655 &
+6.6 \% & –14.4 \% \\
\addlinespace
Refine & \textbf{0.7925} & 0.8350 & \textbf{0.8069} & 
0.6214 & \textbf{0.7407} & 0.6886 &
–21.6 \% & –14.7 \% \\
\bottomrule
\end{tabular}
\end{table*}


\paragraph{Retrieve stage.}
Clause-level retrieval significantly improves the quality of the candidate set compared with article-level retrieval. The baseline RRR framework, when evaluated on COLIEE 2023, reported a precision of only 0.2280 for the Retrieve stage but an extremely high recall of 0.9250. In contrast, our clause-aware Retrieve stage achieves 0.6772 precision and 0.8011 recall, representing nearly a threefold increase in precision with only a moderate decline in recall. This confirms that fine-grained indexing captures legally entailing snippets more effectively and suppresses noise from irrelevant portions of long articles. 

A detailed analysis of retrieved candidates shows that the hybrid dense–sparse retriever recovers both semantically aligned and lexically exact matches. Sparse retrievers (BM25 and SPLADE) excel in locating clauses containing explicit references to statutory sections or terminology, while dense embeddings (LegalBERT) recover paraphrased clauses describing equivalent legal principles. Combining the two with a hybrid scoring scheme produces a balanced candidate pool, which explains the substantial lift in overall precision.

\paragraph{Revise stage.}
In the Revise phase, large language models (LLMs) act as revisers to validate whether a retrieved set of clauses is sufficient to entail the query statement. When transitioning from article-level to clause-level prompts, LLMs receive more localized, information-dense evidence but less textual redundancy. The resulting precision–recall profile reflects this trade-off. Our Revise stage yields a macro F$_2$ of 0.6655, a decline relative to the retrieval stage, but one that indicates the LLMs are making more conservative judgments. In several cases, the LLM rejects correct clause sets if the context is minimal or syntactically complex (for example, clauses containing legal exceptions or multi-sentence conditions). 

Nevertheless, qualitative inspection of the LLM outputs reveals more explainable rationales and a lower incidence of spurious entailments compared to the article-level version of RRR. This suggests that the clause-level representation improves interpretability even if it currently limits recall.

\paragraph{Refine stage.}
The Refine stage applies small-LM agreement and clause-evidence boosting to reconcile the high-recall retrieval output with LLM-validated predictions. In our implementation, the final macro F$_2$ improves slightly to 0.6886. While the gain is modest, the result confirms that reintroducing clause-level frequency information (i.e., rewarding articles supported by multiple entailing clauses) can recover true positives pruned by conservative LLM decisions. Due to fallback logic in our ensemble module, part of the refine output duplicated the revise results; resolving this bug is expected to yield additional precision gains.

\subsection{Comparison with published RRR results}
To contextualize our findings, we compare them with Nguyen et~al.’s reported results on COLIEE 2023. Their full three-stage pipeline achieved an F$_2$ of 0.8069 after the Refine stage, while retrieval alone achieved 0.5598. Our clause-aware system achieves 0.7420 after retrieval—already surpassing the baseline’s retrieval performance by 32~points in absolute F$_2$. Even though our final F$_2$ (0.6886) does not exceed their optimized article-level model, it demonstrates that clause-level modeling substantially raises the lower bound of retrieval quality. 

More importantly, the results demonstrate that retrieval improvements transfer meaningfully to later stages: both Revise and Refine stages operate on smaller, more accurate candidate pools, leading to more stable predictions and potentially lower inference costs for LLMs. :contentReference[oaicite:1]{index=1}

\subsection{Qualitative observations}
Manual analysis of a sample of 50 queries shows that clause-level retrieval often returns multiple short fragments that each partially entail the query. In many such cases, aggregation across these fragments yields a more complete logical explanation than any single article-level retrieval. This observation aligns with our theoretical motivation: legal entailment is compositional, and clause-level retrieval provides the atomic evidence required for transparent reasoning chains.

However, in rare instances where the relevant information is distributed across multiple noncontiguous clauses, our pipeline may underperform if those clauses are not all captured in the top-$k$ candidates. Further work on contextual clause linking and discourse-aware retrieval could mitigate this limitation.

\subsection{Error analysis}
A detailed breakdown of error cases reveals three primary sources of performance loss:
\begin{enumerate}
    \item \textbf{Clause segmentation errors:} Imperfect segmentation can merge semantically unrelated sentences or split a single legal condition mid-sentence, distorting embeddings and hurting dense retrieval accuracy.
    \item \textbf{Prompt interpretation errors:} Some LLMs fail to interpret partially decontextualized clauses, leading to ``not enough information'' responses even for correct entailments.
    \item \textbf{Fallback intersection bias:} The strict intersection rule in our refine ensemble penalizes partially overlapping positive predictions, removing valid candidates that are supported by one subsystem but not another.
\end{enumerate}

\subsection{Key takeaway}
Clause-level retrieval dramatically raises the quality of candidate evidence. Even though downstream LLM validation has yet to fully capitalize on this improvement, the results clearly indicate that fine-grained retrieval constitutes a more reliable and interpretable foundation for legal entailment reasoning.

\begin{figure*}
    \centering
    \includegraphics[width=0.6\linewidth]{image2.png}
    \caption{F2 score comparisons across different methods}
    \label{fig:placeholder}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{image.png}
    \caption{Number of articles in different prompting}
    \label{fig:placeholder}
\end{figure*}
\subsection{Experimental design}
We conducted controlled ablations to isolate the contributions of each major design element of our clause-aware pipeline. The experiments were executed using identical hardware and dataset splits to maintain comparability. Each ablation disables or replaces one of the novel components—clause segmentation, hybrid retrieval, clause-informed prompting, or clause-evidence boosting—while leaving the rest of the pipeline unchanged. 

Metrics are reported for each stage independently, using the same evaluation script as the primary experiment. The ablation results allow us to quantify how much each innovation contributes to overall system performance.

\subsection{Ablation results}
\paragraph{A1: Removing clause segmentation.}
Without segmentation, retrieval operates at the article level using the same hybrid retrieval model. The retrieval F$_2$ drops from 0.7420 to roughly 0.5410, driven primarily by a collapse in precision. The system retrieves large, text-heavy articles that contain minor references to the query topic but also numerous irrelevant sections. This confirms that clause segmentation is the dominant factor behind our precision improvements.

\paragraph{A2: Sparse-only retrieval.}
When the dense retriever (LegalBERT) is disabled and only BM25/SPLADE is used, recall falls sharply (0.8011 $\rightarrow$ 0.6932). While sparse methods are strong for keyword matching, they fail to capture paraphrased legal statements that use alternative terminology. This degradation mirrors observations in other long-document retrieval studies where dense embeddings are essential for semantic generalization.

\paragraph{A3: Dense-only retrieval.}
Using the dense retriever alone yields the opposite behavior: recall remains high, but precision declines because purely semantic embeddings cannot distinguish near-synonyms that appear in irrelevant legal contexts. For example, ``liability'' in one statute may refer to contractual obligations rather than tort law, leading to semantic false positives. Hybridization remains the best balance. \ref{tab:rrr_results}
\begin{table*}[htbp]
\centering
\caption{Clause Retrieval Performance on COLIEE2025 Dataset}
\label{tab:rrr_results}
\begin{tabular}{lcccc}
\toprule
\textbf{Run} & \textbf{Macro Precision} & \textbf{Macro Recall} & \textbf{Macro F2-Score} & \textbf{Correct / Total Predictions} \\
\midrule
R02 & 0.5916 & 0.640 & 0.6133 & 57 / 107 \\
R03 & 0.618 & 0.711 & 0.6621 & 86 / 160 \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{A4: Clause-informed prompting disabled.}
When the LLM receives entire articles without clause-level highlights, both Revise precision and interpretability decline. F$_2$ falls by roughly 0.05 on average. The LLM spends more tokens ``searching'' for the relevant evidence within long texts, often hallucinating justification segments or ignoring relevant sections at the end of documents.

\paragraph{A5: No clause-evidence boosting in Refine.}
Disabling the boosting heuristic reduces final precision from 0.6214 to 0.5820. Recall slightly increases because the ensemble becomes more permissive, but overall F$_2$ drops. The heuristic effectively rewards articles that are supported by multiple strong clauses, providing a proxy for the completeness of legal reasoning within a statute.



\subsection{Few-Shot Enhancement: Qwen Integration}

To further improve reasoning granularity, we incorporated the \textbf{Qwen-1.8B-Chat} \cite{qwen2024} model in a few-shot revise phase, utilizing 50 clause-informed examples. The relaxed refine phase (min\_confidence = 0.3, score\_gap\_threshold = 0.4) aimed to increase recall while maintaining robustness. Table~\ref{tab:fewshot_ablation} summarizes the comparative results.

\begin{table*}[htbp]
\centering
\caption{Ablation Results: Impact of Few-Shot Learning and Relaxed Refinement}
\label{tab:fewshot_ablation}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Precision (\%)} & \textbf{Recall (\%)} & \textbf{F2-Score (\%)} & \textbf{Coverage (\%)} \\
\midrule
Baseline (Retrieve Only) & 59.16 & 64.20 & 61.33 & 100 \\
Basic Revise + Relaxed Refine & 51.75 & 52.47 & 51.41 & 100 \\
Qwen Few-Shot + Relaxed Refine (R02) & 48.87 & 48.15 & 47.33 & 86.4 \\
Qwen Few-Shot + Relaxed Refine (R03, est.) & $\sim$52.50 & $\sim$52.00 & $\sim$52.00 & $\sim$90 \\
Strict Refine & 22.43 & 20.99 & 20.94 & 100 \\
\bottomrule
\end{tabular}
\end{table*}

The results indicate that while the few-shot integration slightly reduces precision, it improves contextual coverage and interpretability. However, query coverage dropped to 86.4\%, suggesting a trade-off between model confidence thresholds and completeness. Adjusting refinement aggressiveness remains an open direction for optimization.

The fine-grained hybrid RRR framework demonstrates steady improvements over previous baselines. The R03 configuration notably balances retrieval accuracy and recall. The few-shot learning variant introduces contextual adaptability, though at the cost of reduced prediction coverage. These results collectively underline the importance of balancing lexical retrieval precision with semantic reasoning and adaptive refinement.




\subsection{Hyperparameter sensitivity}
We tested the sensitivity of the pipeline to three critical hyperparameters: the number of top clauses ($k$) passed to the Revise stage, the number of few-shot exemplars used in prompting, and the top-$r$ parameter controlling the size of the small-LM refiner set.  

\begin{itemize}
    \item Increasing $k$ beyond 5 results in degraded Revise precision, as the LLM must consider increasingly large and redundant clause sets. The optimal value of $k$ for COLIEE 2025 is 3.
    \item For few-shot examples, the sweet spot lies between 10 and 30. Too few examples lead to inconsistent predictions; too many consume context window space and reduce evidence tokens.
    \item For the refiner’s top-$r$, values around 6--8 yield the best trade-off between coverage and precision. Smaller $r$ values reduce robustness to retrieval variance.
\end{itemize}

\subsection{Qualitative ablations}
To complement the numeric results, we performed a qualitative evaluation of LLM outputs for ablated and full configurations. The clause-informed system consistently produced more focused rationales referencing explicit statutory clauses, while the non-clause version often produced abstract reasoning without citation to text. These qualitative differences reinforce the empirical evidence that clause-awareness enhances interpretability and accountability of legal reasoning.

\subsection{Ablation summary}
Across all tests, the combination of clause segmentation and hybrid retrieval explains roughly 70\% of the overall F$_2$ improvement at the retrieval stage. Clause-informed prompting accounts for an additional 10--12\% improvement in the Revise stage, while clause-evidence boosting yields a smaller but consistent 3--5\% gain in the final stage. The additive nature of these effects supports our claim that the clause-aware pipeline provides orthogonal enhancements: one structural (segmentation), one representational (hybrid retrieval), and one reasoning-level (prompting and boosting).

\section{Discussion}
\label{sec:discussion}

\subsection{Clause-level retrieval as a paradigm shift}
Traditional document retrieval in legal entailment tasks assumes that the relevant unit is an entire statute article or case paragraph. However, legal arguments are inherently modular: a single clause can establish or invalidate an entailment independent of its surrounding text. Our experiments confirm that shifting the retrieval granularity from articles to clauses provides a significant benefit. This shift aligns with trends in large-scale question answering and retrieval-augmented generation, where fine-grained evidence retrieval (sentence or passage-level) leads to more interpretable reasoning chains.

By decomposing legal articles into atomic clauses, our model avoids ``topic dilution,'' a phenomenon observed in RRR and other COLIEE systems where long, multi-topic articles overwhelm embedding similarity with unrelated content. Clause-level retrieval surfaces the precise fragments that contain the entailing logic and improves the transparency of subsequent reasoning stages.

\subsection{Why final-stage performance lags behind baseline}
Although our retrieval quality surpasses the baseline, the end-to-end F$_2$ remains lower than the highly optimized RRR results on previous COLIEE years. Several explanations are supported by the evidence:

\begin{enumerate}
    \item \textbf{Dataset distribution shift.} The COLIEE 2025 dataset contains longer statutes and more abstract questions, which naturally lower achievable precision. Comparing across years is therefore not a direct like-for-like benchmark.
    \item \textbf{LLM conservatism.} Clause-level contexts are shorter and sometimes ambiguous without neighboring clauses. LLMs trained on general-purpose corpora tend to err on the side of ``insufficient information,'' producing false negatives. Future work can address this through contextual clause expansion, where adjacent clauses are retrieved dynamically if they provide clarifying conditions.
    \item \textbf{Ensemble fallback.} Due to a safety fallback in our implementation, the Refine stage often defaulted to the Revise output if any model disagreement occurred. This reduced the potential gain from clause-evidence boosting and sLM–LLM consensus logic.
    \item \textbf{Hyperparameter tuning limits.} Time constraints prevented an exhaustive search of prompt templates and few-shot configurations. The RRR paper reports that prompt diversity and adaptive selection of examples significantly improve precision; reproducing such tuning on our dataset is expected to close much of the performance gap.
\end{enumerate}

\subsection{Implications for legal retrieval research}
The success of clause-level retrieval indicates that future legal AI systems should model legal texts not as monolithic articles but as collections of fine-grained, interlinked propositions. This perspective opens several directions:
\begin{itemize}
    \item \textbf{Structured retrieval pipelines:} Clause embeddings can serve as nodes in a graph where edges represent logical dependencies or cross-references, enabling graph-based reasoning.
    \item \textbf{Explainable entailment:} Presenting the specific clauses that justify a decision aligns with legal requirements for interpretability and traceability.
    \item \textbf{Cross-lingual generalization:} Clause segmentation provides a more consistent unit for multilingual legal corpora, facilitating translation-based retrieval and evaluation.
\end{itemize}

\subsection{Recommendations for improvement}
To further improve the Clause-Aware RRR pipeline, we suggest several engineering and modeling enhancements:
\begin{enumerate}
    \item \textbf{Context-aware clause linking.} Introduce a retrieval-time expansion step where clauses are linked to their immediate legal neighbors if they share cross-references or logical connectors (e.g., ``provided that,'' ``except where''). This would reduce the risk of retrieving isolated, context-starved fragments.
    \item \textbf{Structured LLM output formats.} Require the LLM to emit decisions in a structured schema (JSON with explicit entailment labels and rationales), simplifying automatic parsing and reducing misclassification.
    \item \textbf{Verifier ensembles.} Integrate a lightweight entailment verifier (e.g., fine-tuned DeBERTa) that re-scores LLM outputs for logical consistency, as demonstrated in our preliminary tests. 
    \item \textbf{Adaptive refiner weighting.} Replace the current intersection rule with a probabilistic fusion where votes from different models are weighted by confidence and clause support count.
    \item \textbf{Prompt optimization.} Conduct large-scale prompt ablations varying exemplar selection, reasoning style (chain-of-thought vs.\ direct answer), and instruction framing. 
\end{enumerate}

\subsection{Limitations and future directions}
While the current study provides a robust proof of concept for clause-level retrieval, several limitations remain. First, clause segmentation was rule-based and may not align perfectly with legal discourse boundaries. Future work could adopt syntactic or rhetorical segmentation models trained on legal corpora. Second, dense retrievers were initialized from general LegalBERT checkpoints; domain adaptation to specific jurisdictions could further improve representation fidelity. Third, evaluation on COLIEE 2025 alone limits generalizability; applying the same pipeline to other legal datasets such as CaseHOLD or LEDGAR would test its broader applicability.

\subsection{Conclusion of discussion}
In summary, the clause-aware extension of the RRR framework demonstrates that retrieval granularity is a crucial determinant of downstream legal reasoning quality. Our experiments establish a new baseline for fine-grained legal retrieval and provide actionable insights for integrating LLM reasoning with structured, clause-level evidence. Although further engineering and tuning are required to reach the full performance potential observed in earlier RRR results, the gains in interpretability and retrieval precision make the clause-aware approach a strong foundation for future research in evidence-centric legal AI.


\bibliographystyle{acm}
\bibliography{sample-base}

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.



\section{Tables}

The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
high-quality tables.

Table captions are placed {\itshape above} the table.

Because tables cannot be split across pages, the best placement for
them is typically the top of the page nearest their initial cite.  To
ensure this proper ``floating'' placement of tables, use the
environment \textbf{table} to enclose the table's contents and the
table caption.  The contents of the table itself must go in the
\textbf{tabular} environment, to be aligned properly in rows and
columns, with the desired horizontal and vertical rules.  Again,
detailed instructions on \textbf{tabular} material are found in the
\textit{\LaTeX\ User's Guide}.

Immediately following this sentence is the point at which
Table~\ref{tab:freq} is included in the input file; compare the
placement of the table here with the table in the printed output of
this document.

\begin{table}
  \caption{Frequency of Special Characters}
  \label{tab:freq}
  \begin{tabular}{ccl}
    \toprule
    Non-English or Math&Frequency&Comments\\
    \midrule
    \O & 1 in 1,000& For Swedish names\\
    $\pi$ & 1 in 5& Common in math\\
    \$ & 4 in 5 & Used in business\\
    $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
  \bottomrule
\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of the page's
live area, use the environment \textbf{table*} to enclose the table's
contents and the table caption.  As with a single-column table, this
wide table will ``float'' to a location deemed more
desirable. Immediately following this sentence is the point at which
Table~\ref{tab:commands} is included in the input file; again, it is
instructive to compare the placement of the table here with the table
in the printed output of this document.

\begin{table*}
  \caption{Some Typical Commands}
  \label{tab:commands}
  \begin{tabular}{ccl}
    \toprule
    Command &A Number & Comments\\
    \midrule
    \texttt{{\char'134}author} & 100& Author \\
    \texttt{{\char'134}table}& 300 & For tables\\
    \texttt{{\char'134}table*}& 400& For wider tables\\
    \bottomrule
  \end{tabular}
\end{table*}



